# LLM Time Decay: The 2.25 Recursion Plateau Discovery

## ðŸ”¬ Major Finding: Peak at Fractional Depth 2.25

**UPDATE (Sept 19, 2025):** Fine-grained mapping reveals the true peak is at depth ~2.25, not 2.0!

### Key Discoveries:
- **Initial observation**: Coarse sampling suggested peak at depth 2.0
- **Fine mapping**: Revealed actual peak at 2.23 (GPT) and 2.26 (Claude)  
- **Plateau structure**: High coherence maintained across depths 2.0-2.3
- **Model fits**: Gaussian+decay (RÂ² > 0.80) beats exponential (RÂ² < 0.62)

This suggests LLMs process recursion along a **continuum** rather than discrete levels.

## ðŸš€ Current Status

### Completed Experiments:
- âœ… Initial fractional depth testing (n=1500)
- âœ… Peak mapping [1.6-2.4] in 0.1 increments (n=540)
- âœ… Statistical confirmation of Gaussian+decay model

### In Progress:
- ðŸ”„ Ultra-fine mapping around 2.25 (0.05 increments)
- ðŸ”„ Testing additional models (GPT-4, Claude-opus)

## ðŸ“Š Repository Structure

```
llm-time-decay/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # Experiment configurations
â”‚   â”œâ”€â”€ generator.py        # Prompt generation with fractional depths
â”‚   â”œâ”€â”€ scorer.py           # Coherence scoring system
â”‚   â”œâ”€â”€ runner.py           # Main experiment runner
â”‚   â””â”€â”€ analyzer.py         # Statistical analysis
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw API responses
â”‚   â””â”€â”€ processed/          # Analyzed results
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/            # Visualizations
â”‚   â”‚   â”œâ”€â”€ decay_curves.png
â”‚   â”‚   â”œâ”€â”€ depth2_anomaly.png
â”‚   â”‚   â””â”€â”€ peak_mapping_*.png
â”‚   â””â”€â”€ tables/             # Statistical outputs
â”œâ”€â”€ peak_mapping_experiment.py  # Peak analysis tools
â”œâ”€â”€ paper_outline.md        # Draft paper structure
â””â”€â”€ README.md               # This file
```

## ðŸ§ª Replication Instructions

### Setup
```bash
pip install -r requirements.txt
cp .env.example .env
# Add API keys to .env
```

### Run Peak Mapping
```bash
# Fine mapping around 2.25
python3 src/runner.py --models gpt-3.5 claude-3-haiku --depths peak_mapping --trials 30

# Analyze results
python3 peak_mapping_experiment.py --data data/processed/results_*.json --plot
```

## ðŸ“ˆ Key Results

| Model | Peak Location | Peak Width (Ïƒ) | Gaussian RÂ² | Exponential RÂ² |
|-------|--------------|----------------|-------------|----------------|
| GPT-3.5 | 2.23 Â± 0.05 | 0.38 | 0.898 | 0.619 |
| Claude-3-haiku | 2.26 Â± 0.05 | 0.25 | 0.796 | 0.609 |

## ðŸ”— Related Work

- Original temporal coherence study: [temporal-coherence-llm](https://github.com/HillaryDanan/temporal-coherence-llm)
- Discovered architectural differences between GPT and Claude
- This work extends to find universal fractional-depth optimum

## ðŸ“ Citation

```bibtex
@article{danan2025fractional,
  title={The 2.25 Recursion Plateau: Fractional Optimal Depth in LLM Metacognition},
  author={Danan, Hillary and Claude},
  year={2025},
  journal={arXiv preprint},
  note={In preparation}
}
```

## ðŸ¤ Collaboration

Human-AI research partnership:
- **Hillary Danan**: Experimental design, execution, analysis
- **Claude**: Hypothesis refinement, statistical framework, interpretation

---

*"The best discoveries are the ones that violate your assumptions"*

## ðŸš€ Current Mission

Testing fine-grained temporal depths (0.5 increments) to determine if:
1. GPT uses continuous attention decay
2. Claude uses discrete processing stages
3. Why the fuck they're so different

## ðŸ“Š Repository Structure

```
llm-time-decay/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # API keys, model configs
â”‚   â”œâ”€â”€ generator.py        # Fractional depth prompts
â”‚   â”œâ”€â”€ scorer.py           # Enhanced scoring system
â”‚   â”œâ”€â”€ runner.py           # Main experiment runner
â”‚   â””â”€â”€ analyzer.py         # Statistical analysis
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw API responses
â”‚   â””â”€â”€ processed/          # Scored results
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/            # Visualizations
â”‚   â””â”€â”€ tables/             # Statistical outputs
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ validation.py       # Quality control
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš¡ Quick Start

```bash
# Setup
pip install -r requirements.txt
cp .env.example .env
# Add your API keys to .env

# Run experiment
python3 src/runner.py --depths fractional --models all --trials 50

# Analyze results
python3 src/analyzer.py --hypothesis discrete-vs-continuous
```

## ðŸ§ª Experimental Design

**Testing Matrix:**
- Depths: [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]
- Models: GPT-3.5-turbo, Claude-3-haiku, Gemini-1.5-flash
- Trials: 50 per condition (n=1500 total with all 3 models)
- Metrics: Temporal ordering, causal maintenance, depth accuracy, transition smoothness

## ðŸ“ˆ Expected Outcomes

| Hypothesis | GPT Pattern | Claude Pattern | Gemini Pattern | Interpretation |
|------------|-------------|----------------|----------------|----------------|
| Continuous | Smooth exponential | Smooth exponential | Smooth exponential | Original = sampling artifact |
| Discrete | Step functions | Step functions | Step functions | All LLMs quantize time |
| Divergent | Smooth exponential | Step functions | ??? | ARCHITECTURAL DIFFERENCE |

## ðŸ”— Related Work

Building on: [temporal-coherence-llm](https://github.com/HillaryDanan/temporal-coherence-llm)

## ðŸ“ Citation

```bibtex
@article{danan2025architectural,
  title={Fine-Grained Analysis of Architectural Divergence in LLM Temporal Processing},
  author={Danan, Hillary and Claude},
  year={2025}
}
```

## ðŸ¤ Collaboration

Human-AI research partnership:
- **Hillary**: Experimental design, execution, analysis
- **Claude**: Hypothesis development, statistical framework, implementation

---

*"The best discoveries are the ones you didn't expect to make"*